{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_sentiment_with_spark_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08r3y2HQf0Dj"
      },
      "source": [
        "## Sentiment Analysis with [Spark NLP](https://nlp.johnsnowlabs.com/?gclid=CjwKCAjwr7X4BRA4EiwAUXjbt8SXPLqhOytb-o6ZpGC67FuhfJkiaI3GR2EvdTItYmQXEK2gIRfmlBoCzt8QAvD_BwE)\n",
        "\n",
        "\n",
        "In this notebook, we will use Spark NLP, an industry level open source NLP library. After implementing the preprocessing steps, we will use the pretrained sentiment_analyzer from Spark NLP use it to analyze the sentiments in our Twitter dataset. Our goal is to introduce you to one of the most robust NLP tools and libraries that you can continue learning more about as you keep experimenting with NLP techniques. \n",
        "\n",
        "\n",
        "*Please note, in order to have a full grasp of Spark NLP, as well as any other NLP library or tool, you will first need to get familiarized with their documentation and concepts. To learn more about Spark NLP visit the [documentation](https://nlp.johnsnowlabs.com/docs/en/concepts)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxnWejbbf0Dk"
      },
      "source": [
        "+ We will first setup the necessary colab environment. \n",
        "  + Run this block only if you are inside Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0P75ZK8f0Dk"
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==2.4.4\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp==2.6.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhjgcoR1f0Ds"
      },
      "source": [
        "+ Next, we will mount Google colab by running the cell below and clicking on the URL to get the authorization code. \n",
        "  + If you are coding along, copy and paste your authorization code from the url that appears after you run the cell below to the provided box. If you are using Jupyter Notebook, you don't need to do this step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YJ2HNYwf0Ds"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"content\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l372akSCf0Dv"
      },
      "source": [
        "+ We have now set up our environment on Google colab and can continue with the next steps, using Spark NLP to do sentiment analysis. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkQzQNBxkEA8"
      },
      "source": [
        "### 1. Sentiment Analysis Using a pretrained Pipeline\n",
        "\n",
        "Using a pipeline with spark dataframes, we can also use the pipeline through a spark dataframe. We first need to create a spark dataframe with a column named “text” that will work as the input for the pipeline and then use the `.transform()` method to run the pipeline over that dataframe and store the outputs of the different components in a spark dataframe.\n",
        "\n",
        "In this example, we are not doing any training or using a model that we created, but we are simply using Spark NLP, out of the box, to tell us what the sentiment of a text that we give to it is. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw8WVBuzkDHX"
      },
      "source": [
        "import sys\n",
        "import sparknlp\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import array_contains\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.pretrained import PretrainedPipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PFNzn24f0Dp"
      },
      "source": [
        "+ We will now start a spark nlp session, as well as check for versions of both Apache Spark and Spark NLP. Running this cell without an error means we have installed the necessary packages correctly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfeOWeGykDFL"
      },
      "source": [
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipyp-gM4o0C7"
      },
      "source": [
        "+ Load the predefined pipeline provided in Spark NLP containing all the annotators we need to run a sentiment analysis on a piece of raw text.\n",
        "\n",
        "+ + The next step in the process is to initialize the pretrained model from Spark NLP. For sentiment analysis, we will use the named `analyze_sentiment` for the English language. \n",
        "\n",
        "+ In this example, we can simply use a text that could be provided by a user, a client, or any piece of text that you would like to get the sentiment associated to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuxtGi9WkDCu"
      },
      "source": [
        "pipeline = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PVQxXMVpAF6"
      },
      "source": [
        "+ Create random list of sentences that you would like the model to analyze. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXPGIsbQkC-M"
      },
      "source": [
        "dataset = [\"I’m delighted to be part of the change—the change being more women visibly succeeding because I think that’s an important way in which we encourage the next generation. #WeekendInspiration from #womeninSTEM and Nobel Prize winners in chemistry & physics are women!\"]\n",
        "\n",
        "# Alternatively, you can put this tiny data into a spark dataframe\n",
        "# data = spark.createDataFrame([[\"I’m delighted to be part of the change—the change being more women visibly succeeding because I think that’s an important way in which we encourage the next generation. #WeekendInspiration from #womeninSTEM and Nobel Prize winners in chemistry & physics are women!\"]]).toDF('text')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gjSZOfbkC73"
      },
      "source": [
        "# Annotate our tiny dataset\n",
        "result = ...\n",
        "[(r['sentence'], r['sentiment']) for r in result]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzbNs8geqFcy"
      },
      "source": [
        "# We can also view each stage in the pipeline by simply printing it.\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rianiXpLrpHR"
      },
      "source": [
        "### 2. Sentiment Analysis Using A Pretrained Model, SentimentDL, to Train on Our Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKvDbTlrx4VC"
      },
      "source": [
        "import time\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import DocumentAssembler, Finisher\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q7adCNj4xj-"
      },
      "source": [
        "+ Let's pull in our twitter dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTmfS8Wp2hxv"
      },
      "source": [
        "df = pd.read_csv('/content/content/My Drive/anitab-nlp-workshop/data/election.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIGB-yw44nEw"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPy42SY0XHL0"
      },
      "source": [
        "+ Pretrained pipelines expect the input column to be named **text**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWhIou0YHW4a"
      },
      "source": [
        "df = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb_2SmCgHbhZ"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neC3XLuUpbQ_"
      },
      "source": [
        "**Splitting the Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59eKQIurXgZw"
      },
      "source": [
        "+ Split the dataset into train and test sets, save these subsets into two different csv files, using pandas and numpy\n",
        "  + This can also be done with `scikit-learn` library. This is simply another way of splitting our data if you are trying to reduce the overhead of your code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJi6LfW-2h0u"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Randomly select %80 of the dataset and use it for training. \n",
        "mask = np.random.rand(len(df)) < 0.8\n",
        "trainDataset = ...\n",
        "\n",
        "# Take the complement of the training set we have split above (i.e %20 of the data for testing).\n",
        "testDataset = ...\n",
        "\n",
        "#save these subsets (train & test) into csv\n",
        "trainDataset.to_csv('/content/content/My Drive/anitab-nlp-workshop/data/trainDataset.csv', index=False)\n",
        "testDataset.to_csv('/content/content/My Drive/anitab-nlp-workshop/data/testDataset.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ67TOkl53yo"
      },
      "source": [
        "+ See how many rows of data we have in training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4UPBbtX8D-U"
      },
      "source": [
        "trainDataset.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFToT0Pb9GL2"
      },
      "source": [
        "testDataset. ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxy_fNfT8kbo"
      },
      "source": [
        "trainDataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl59bYA-57mW"
      },
      "source": [
        "+ Convert the data into a pyspark dataframe to make it compatible with Spark NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "che6inNi-Cvx"
      },
      "source": [
        "spark_train = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEpX3Qy4y7h-"
      },
      "source": [
        "spark_test = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xwBXuHpdn4r"
      },
      "source": [
        "spark_train.show(n=10, truncate=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVcFo-pS6Pc0"
      },
      "source": [
        "+ Setup the Pipeline for the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pusEZ5rLf0EB"
      },
      "source": [
        "+ With any new tool or library libray, there is often some specific terminology that you need to learn. In this case, the term we need to pay attention to is \"pipeline,\"\n",
        "    + *In Machine Learning, a pipeline is often defined as a sequence of algorithms to process and learn from data. It is a sequence of stages, and in Spark NLP, each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. That is, the data is passed through the fitted pipeline in order. For more details on Spark Pipelines that Spark NLP uses, please visit [here](http://spark.apache.org/docs/latest/ml-pipeline.html).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz2sVAXP9inl"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JMbSmh19iqT"
      },
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "use = UniversalSentenceEncoder.pretrained() \\\n",
        " .setInputCols([\"document\"])\\\n",
        " .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "# the classes/labels/categories are in sentiment column\n",
        "sentimentdl = SentimentDLApproach()\\\n",
        "  .setInputCols([\"sentence_embeddings\"])\\\n",
        "  .setOutputCol(\"class\")\\\n",
        "  .setLabelColumn(\"sentiment\")\\\n",
        "  .setBatchSize(64)\\\n",
        "  .setMaxEpochs(5)\\\n",
        "  .setEnableOutputLogs(True)\n",
        "\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        use,\n",
        "        sentimentdl\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM74r_bU6hJ_"
      },
      "source": [
        "+ Train the model on our training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPq2Ar9V9iu5"
      },
      "source": [
        "pipelineModel = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTXSbx08w6R3"
      },
      "source": [
        "!cd ~/annotator_logs && ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm1XeB0_xu-x"
      },
      "source": [
        "#!cat ~/annotator_logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRlGJea6SqIA"
      },
      "source": [
        "### Save and load pre-trained SentimentDL model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYzum7o4Sj1e"
      },
      "source": [
        "pipelineModel.stages[-1].write().overwrite().save('./my_sentimentdl_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d2uKuhm0nU-"
      },
      "source": [
        "+ Use our pre-trained SentimentDLModel in a pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_99ySASW6vZm"
      },
      "source": [
        "# In a new pipeline we can now load our model for prediction\n",
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "use = UniversalSentenceEncoder.pretrained() \\\n",
        " .setInputCols([\"document\"])\\\n",
        " .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "sentimentdl = SentimentDLModel.load(\"./my_sentimentdl_model\") \\\n",
        "  .setInputCols([\"sentence_embeddings\"])\\\n",
        "  .setOutputCol(\"class\")\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        use,\n",
        "        sentimentdl\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88kkBKbA4E_T"
      },
      "source": [
        "from pyspark.sql.types import StringType\n",
        "\n",
        "test_data = spark.createDataFrame([\n",
        "    \"This election will be interesting!\",\n",
        "    \"I voted!\"\n",
        "], StringType()).toDF(\"text\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67Yc7_K44FDU"
      },
      "source": [
        "prediction = ...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UevsNPfC4FG8"
      },
      "source": [
        "prediction.select(\"class.result\").show()\n",
        "\n",
        "prediction.select(\"class.metadata\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC1i4QhIR2U-"
      },
      "source": [
        "## Evaluation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULdjRAzvR5fk"
      },
      "source": [
        "Similar to other NLP libraries, we can use the evaluation metrics for NLP, evaluating our Spark NLP sentimentdl model. For this, we will first run the model on our test set. We leave it to you for practice to experiment with evaluations metrics in `scikit-learn` library. (Hint: Revisit Part I notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f6sFBY0SIjc"
      },
      "source": [
        "predictions = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atoRFBMdabS0"
      },
      "source": [
        "predictions.select('sentiment','text',\"class.result\").show(30, truncate=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhA7i86S5ktY"
      },
      "source": [
        "+ SentimentDL has the ability to accept a threshold to set a label on any result that is less than that number. By default the threshold is set on 0.6 and everything below that will be assigned as neutral. You can change this label with `setThresholdLabel` attribute.\n",
        "\n",
        "+ We need to filter neutral results since we don't have any in the original test dataset to compare with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-uTYNhnabYb"
      },
      "source": [
        "preds = predictions.select('sentiment','text',\"class.result\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxoSJkYJyx83"
      },
      "source": [
        "preds['result'] = ...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz6RxXSwpxuc"
      },
      "source": [
        "preds = preds[preds['result'] != 'neutral']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntwy6EtZ6TLc"
      },
      "source": [
        "preds.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXj_ZNN154NH"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "print (...)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}