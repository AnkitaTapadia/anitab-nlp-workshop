{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will take you through some common steps to clean and prepare your data in preparation for training and testing your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, if you are running this notebook in Google Colab, the following 3 cells needs to be executed to run the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "LKVBPMdNcWh0",
    "outputId": "0e436feb-c5e1-4224-d31d-5a1137839ece"
   },
   "outputs": [],
   "source": [
    "#install the appropriate packages\n",
    "import os\n",
    "\n",
    "# Install java\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install --ignore-installed pyspark==2.4.4\n",
    "\n",
    "# Install Spark NLP\n",
    "! pip install --ignore-installed spark-nlp==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give permission to colab to access files from your google drive.  This permission lasts for just this Colab session.\n",
    "#You will need to click on the link that pops up, give Colab permission, and copy and paste the string into the box\n",
    "#that pops up down below.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "9aE_a9r5cNSM",
    "outputId": "f2d815a0-bacb-4d75-ac37-f92166cc7be7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import sparknlp and confirm all is installed properly with the following commands\n",
    "\n",
    "import sparknlp\n",
    "# Start Spark Session with Spark NLP\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version\")\n",
    "sparknlp.version()\n",
    "print(\"Apache Spark version\")\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzLz3dicFqzj"
   },
   "source": [
    "## Steps to pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wf0fteXYFqzj"
   },
   "source": [
    "Steps 1-3 are some typical steps taken to clean and process the data to prepare our features (step 4).\n",
    "\n",
    "1. Tokenize\n",
    "2. Remove stop words\n",
    "3. Perform stemming/lemmatization\n",
    "4. Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOO8E9cVFqzk"
   },
   "source": [
    "Today, we're going to be working with a text loaded in the following cell for all our pre-processing steps.  The python package, Pandas, is a convenient way to read in the data and use it in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AO4pzbYOFqzk"
   },
   "outputs": [],
   "source": [
    "#this root is set assuming you are using Google Colab.  If you are not, you can set it to root = './data/'\n",
    "root = '/content/content/My Drive/Colab Notebooks/anitab-nlp-workshop/data/'\n",
    "\n",
    "data_path = f'{root}preprocess_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Y8SA303GZcV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELvlScJUFqzo"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path, sep='\\n', header = None, names=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQYT9_TJFqzs"
   },
   "source": [
    "### Data Prep before pre-processing\n",
    "\n",
    "Before we can do any of our pre-prcessing steps with Spark NLP, we must put our data into the proper format.  This involves casting our pandas dataframe to a Spark dataframe, and then creating a document object out of each text in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "-ve7QoNIpmo5",
    "outputId": "f9ba1a92-2d3c-4122-b2cb-bc55d71e4952"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XERMYs0lp55F"
   },
   "outputs": [],
   "source": [
    "#Cast our pandas dataframe into a spark dataframe\n",
    "spark_df = spark.createDataFrame(data.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "m0AtV8eLp8L2",
    "outputId": "ac2d50ba-e63d-4233-b45f-625b1a3ef85a"
   },
   "outputs": [],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xo4j7D9yeja5"
   },
   "outputs": [],
   "source": [
    "#Use the DocumentAssembler to create document objects our of texts in our dataset\n",
    "from sparknlp.base import DocumentAssembler\n",
    "\n",
    "#To do: \n",
    "#1. Instantiate DocumentAssembler \n",
    "documentAssembler = DocumentAssembler()\n",
    "\n",
    "#2. Specify input column\n",
    "documentAssembler.setInputCol('text')\n",
    "\n",
    "#3. Specify output column\n",
    "documentAssembler.setOutputCol('document')\n",
    "\n",
    "#4. Transform the spark dataframe\n",
    "doc_df=documentAssembler.transform(spark_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dataframe that is ready to pre-process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "ubCIR_66Hr8Z",
    "outputId": "ca31d6dd-cf6a-4408-8f4c-68ca600f3cdf"
   },
   "outputs": [],
   "source": [
    "doc_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEAWiNiZFqz4"
   },
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "28jFCQKgFqz5"
   },
   "source": [
    "**Tokenization**: Segmentation of text into words (a form of feature extraction)\n",
    "<div align=\"center\">\n",
    "  <img height = 400, width = 400, src=\"../images/tokenize4.jpg\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i59-mk-WFqz5"
   },
   "source": [
    "Spark NLP has a single function for tokenization that is very convenient to use.  It has several different parameters that can be set in order to change how you do tokenization, all listed within the one tokenization function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pNW9xT0hFqz5"
   },
   "source": [
    "First, we will simply tokenize our text by splitting up the sentence into a list of words, symbols and numbers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2gZl1EmYFqz8"
   },
   "outputs": [],
   "source": [
    "#To do:  Instantiate tokenizer and set columns, tokenize documents.\n",
    "\n",
    "from sparknlp.annotator import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "#set columns\n",
    "tokenizer.setInputCols(['document'])\n",
    "tokenizer.setOutputCol('token')\n",
    "\n",
    "#fit and transform\n",
    "token_df=tokenizer.fit(doc_df)\n",
    "token_df = token_df.transform(doc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "l367EIY4IDKt",
    "outputId": "45c7e46e-e93c-41c8-881e-b1d9d60c2b12"
   },
   "outputs": [],
   "source": [
    "token_df.select('token.result').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rl6x1aBfFq0B"
   },
   "source": [
    "You can also use a regex (or regular expression) to define a pattern by which to tokenize.  In Spark NLP, you would simply call `setTargetPattern` to indicate the regex you want to use. The following example will show how to exclude punctuation from our tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dKPqp2weFq0C"
   },
   "outputs": [],
   "source": [
    "#Use the same tokenizer instance, but now give it a target pattern to split on.\n",
    "tokenizer.setTargetPattern('\\w+')\n",
    "token_df=tokenizer.fit(doc_df)\n",
    "token_df = token_df.transform(doc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "so9fVJ_kI18X",
    "outputId": "9dc3d8c0-c5d7-4352-db3d-48230e00b63c"
   },
   "outputs": [],
   "source": [
    "token_df.select('token.result').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HVkciiIzFq0G"
   },
   "source": [
    "### 2. Remove Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MO5yK2kHFq0G"
   },
   "source": [
    "Removal of words that are not important from the information point of view, such as: 'the', 'is', 'a', etc, can greatly improve your results with your model.  You may have specific words that you know will occur frequently in your text, but you don't actually care about keeping those words in your corpus.  Whatever the case may be, you can provide a list of stop words to Spark NLP's `StopWordsCleaner()` in order to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVYEkrN5Fq0H"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'{root}stopwords.txt', 'rb') as file:\n",
    "    stopwords = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeUhzWOYFq0I"
   },
   "outputs": [],
   "source": [
    "#To do: create StopWordsCleaner instance and remove stop words from tokens.\n",
    "from sparknlp.annotator import StopWordsCleaner\n",
    "\n",
    "stop_words_cleaner = StopWordsCleaner() \n",
    "stop_words_cleaner.setInputCols([\"token\"])\n",
    "stop_words_cleaner.setOutputCol(\"cleanTokens\")\n",
    "stop_words_cleaner.setCaseSensitive(False)  #You may or may not care about case.\n",
    "stop_words_cleaner.setStopWords(stopwords)\n",
    "\n",
    "clean_token_df=stop_words_cleaner.transform(token_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fZJUeFA3J8DN",
    "outputId": "126accb7-2b0c-42f3-f194-79903693a188"
   },
   "outputs": [],
   "source": [
    "clean_token_df.select('cleanTokens.result').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YtoEaq-JFq0K"
   },
   "source": [
    "### 3. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lh-LhwqvFq0K"
   },
   "source": [
    "**Stemming**: Reduces words to their root, but the root might not always result in an actual word.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img height = 300, width = 300, src=\"../images/stem2.jpg\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0UIcMDUrFq0L"
   },
   "source": [
    "\n",
    "Spark NLP has a single stemmer function.  You can assume that the stemmer is up to the latest standards in stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eQeAWtEOFq0L"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Stemmer\n",
    "\n",
    "stemmer = Stemmer()\n",
    "stemmer.setInputCols([\"cleanTokens\"]) \n",
    "stemmer.setOutputCol(\"stem\")\n",
    "\n",
    "stem_df=stemmer.transform(clean_token_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "I1TpHgT3KM2G",
    "outputId": "0e400e60-b2d7-49e9-8bae-1634c5d24fbf"
   },
   "outputs": [],
   "source": [
    "stem_df.select('stem.result').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PNqT9Au4Fq0N"
   },
   "source": [
    "Note: In some cases, you may need to do more than stemming. There is a process called \"lemmatization\" that reduces a word to a root in a more sophisticated way. A lemmatizer will set a past tense verb to present tense, for example.\n",
    "\n",
    "In addition, there is a function called `normalizer` which does several things at once. Per the Spark NLP documentation, a normalizer 'removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary.'  In other words, it can remove punctuation and reduce words to a root based off of a dictionary provided by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cbmwDtgYFq0N"
   },
   "source": [
    "### 4. Word Embedding: Representing Text as Numerical Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TqeMMkSWFq0N"
   },
   "source": [
    "+ We first need to represent texts to numbers that the learning algorithm can process. \n",
    "+ To represent each word in the dataset, we will use the pre-trained `WordEmbeddingsModel` from Spark NLP library. \n",
    "\n",
    "Word embedding converts a text into a numerical vector.  There are several different methods to do this.  The WordEmbeddingsModel is a pre-trained model based on the [GloVe](https://nlp.stanford.edu/projects/glove/) algorithm.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7675DfrHFq0O"
   },
   "source": [
    "<div align=\"center\">\n",
    "      <img height = 350, width = 350, src=\"../images/one_hot2.jpg\">\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "_zuqDgCTFq0O",
    "outputId": "7dd84053-72d6-4a5c-c0df-0b0807c1720c"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import WordEmbeddingsModel\n",
    "\n",
    "word_embeddings=WordEmbeddingsModel.pretrained()\n",
    "word_embeddings.setInputCols(['document','stem'])\n",
    "word_embeddings.setOutputCol('embeddings')\n",
    "\n",
    "embeddings_df=word_embeddings.transform(stem_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "M5yWflGkOAUq",
    "outputId": "31ad071b-0a52-4f1d-c190-8ad9ad065183"
   },
   "outputs": [],
   "source": [
    "embeddings_df.select('embeddings.embeddings').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VePdHOlFq0Q"
   },
   "source": [
    "# Using a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WI2UU-J0Fq0Q"
   },
   "source": [
    "Now, let's process our actual dataset. With this, we can make this process even more streamlined by setting up a **pipeline**.  A pipeline is a set of actions set by the user in a specific order.  The pipeline will take in the dataframe and run through all the steps that you told it to do in an efficient manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "ftDAgUPrFq0Q",
    "outputId": "53635414-9af2-463d-c116-0ba1d19ba45f"
   },
   "outputs": [],
   "source": [
    "#load in the covid19 tweet dataset. \n",
    "data_path = f\"{root}election.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "df = df.rename(columns={'tweet': 'text'})\n",
    "\n",
    "#we need to cast this pandas dataframe as a spark dataframe.\n",
    "spark_df = spark.createDataFrame(df.astype(str))\n",
    "\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZI2i7S-sJBf"
   },
   "outputs": [],
   "source": [
    "#import the Pipeline function\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#Every other instance needed for pre-processing have already been imported and set up.\n",
    "#documentAssembler\n",
    "#tokenizer\n",
    "#stop_words_cleaner\n",
    "#stemmer\n",
    "#word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k-cqevfjsiNm"
   },
   "outputs": [],
   "source": [
    "#To do: set up the pipeline\n",
    "nlpPipeline = Pipeline(stages=[\n",
    "    documentAssembler, \n",
    "    tokenizer,\n",
    "    stop_words_cleaner,\n",
    "    stemmer,\n",
    "    word_embeddings,\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxfSXyQ9sbHj"
   },
   "outputs": [],
   "source": [
    "#create the model specified by the pipeline, feed the Spark dataframe into it.\n",
    "pipelineModel = nlpPipeline.fit(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "joC8Gkkzs1Zb",
    "outputId": "f5b6ba9a-bd7f-4142-9598-0b8ad2ed4637"
   },
   "outputs": [],
   "source": [
    "#obtain results of the pipeline\n",
    "result = pipelineModel.transform(spark_df)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VL_iLtsotffa",
    "outputId": "d5c27153-fb03-47fa-9358-1955a3f49a64"
   },
   "source": [
    "Notice how fast this was to process!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sparknlp_twitter_solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
